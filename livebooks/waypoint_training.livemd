# Reinforcement Learning

```elixir
my_app_root = Path.join(__DIR__, "..")

Mix.install(
  [
    {:boat_learner, path: my_app_root, env: :dev}
  ],
  config_path: Path.join(my_app_root, "config/config.exs"),
  lockfile: Path.join(my_app_root, "mix.lock")
)
```

## Section

```elixir
alias VegaLite, as: Vl

obstacles =
  Nx.tensor([
    [-20, 20, 220, 250],
    [-20, -18, 0, 180],
    [18, 20, 0, 180]
    # [2, 10, 40, 100],
    # [-20, -9, 100, 110]
  ])

target_waypoint = Nx.tensor([4, 160])

obstacles_data =
  obstacles
  |> Nx.to_flat_list()
  |> Enum.chunk_every(4)
  |> Enum.flat_map(fn [x, x2, y, y2] -> [{:x, x}, {:x2, x2}, {:y, y}, {:y2, y2}] end)
  |> Enum.group_by(&elem(&1, 0), &elem(&1, 1))

min_x = -20
max_x = 20

min_y = 0
max_y = 250

widget =
  Vl.new(width: 600, height: 600)
  |> Vl.layers([
    Vl.new()
    |> Vl.data(name: "trajectory")
    |> Vl.mark(:line, opacity: 0.75, tooltip: [content: "data"])
    |> Vl.encode_field(:x, "x", type: :quantitative, scale: [domain: [min_x, max_x]])
    |> Vl.encode_field(:y, "y", type: :quantitative, scale: [domain: [min_y, max_y]])
    |> Vl.encode_field(:color, "episode", type: :nominal, scale: [scheme: "blues"], legend: false)
    |> Vl.encode_field(:order, "index"),
    Vl.new()
    |> Vl.data_from_values(obstacles_data)
    |> Vl.mark(:rect, color: "grey", grid: true)
    |> Vl.encode_field(:x, "x", type: :quantitative)
    |> Vl.encode_field(:x2, "x2", type: :quantitative)
    |> Vl.encode_field(:y, "y", type: :quantitative)
    |> Vl.encode_field(:y2, "y2", type: :quantitative),
    Vl.new()
    |> Vl.data_from_values(%{
      x: [Nx.to_number(target_waypoint[0])],
      y: [Nx.to_number(target_waypoint[1])]
    })
    |> Vl.mark(:circle,
      color: "red",
      opacity: 1,
      grid: true,
      size: [expr: "height * 10 * 10 * #{:math.pi()} /#{max_y - min_y}"]
    )
    |> Vl.encode_field(:x, "x", type: :quantitative)
    |> Vl.encode_field(:y, "y", type: :quantitative)
  ])
  |> Kino.VegaLite.new()
  |> Kino.render()
```

```elixir
reward_callback = fn state_vector ->
  x = state_vector[0]
  y = state_vector[1]
  angle = state_vector[2]
  speed = state_vector[3]
  target_x = state_vector[4]
  target_y = state_vector[5]

  distance = Scholar.Metrics.Distance.euclidean(Nx.stack([x, y]), Nx.stack([target_x, target_y]))
  projected_speed = Nx.sin(angle) |> Nx.multiply(speed)

  Axon.Activations.tanh(Nx.divide(projected_speed, Nx.add(distance, 1.0e-7)))
end
```

```elixir
# 250 max_iter * 15 episodes
max_points = 250 * 15

plot_trajectory = fn axon_state ->
  episode = axon_state.epoch
  num_points = Nx.to_number(axon_state.step_state.iteration)

  trajectory = axon_state.step_state.trajectory

  if num_points > 0 and rem(episode, 500) == 0 do
    x = Nx.to_flat_list(trajectory[[0..(num_points - 1), 0]])
    y = Nx.to_flat_list(trajectory[[0..(num_points - 1), 1]])

    points =
      [x, y]
      |> Enum.zip_with(fn [x, y] -> %{x: x, y: y, episode: episode} end)
      |> Enum.with_index(fn m, idx -> Map.put(m, :index, idx) end)

    Kino.VegaLite.push_many(widget, points, window: max_points, dataset: "trajectory")
  end

  axon_state
end
```

```elixir
Kino.VegaLite.clear(widget, dataset: "trajectory")

{t, _} =
  :timer.tc(fn ->
    BoatLearner.Navigation.WaypointWithObstacles.train(
      obstacles,
      target_waypoint,
      plot_trajectory,
      &BoatLearner.Navigation.WaypointWithObstacles.reward_for_state/1,
      num_episodes: 10_000
    )
  end)

"#{t / 1_000} ms"
```
